{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 3: Bilinear Inverse Problems and Low-Rank Matrix Recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x] By tick the checkbox, we hereby declare that this coursework report is our own and autonomous work. We have acknowledged all material and sources used in its preparation, including books, articles, reports, lecture notes, internet software packages, and any other kind of document, electronic or personal communication. This work has not been submitted for any other assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Test Data Generation (10%)\n",
    "\n",
    "We consider the low-rank matrix completion problem given by \n",
    "$$\n",
    "    \\bm{y} = \\mathcal{P}_{\\Omega}(\\bm{X}) \n",
    "$$\n",
    "where $\\bm{X} \\in \\mathbb{R}^{m \\times n}$ is a low rank matrix of rank $r$. \n",
    "\n",
    "Data generation: Write $\\bm{X} = \\bm{U} \\bm{G} \\bm{V}^{\\mathsf{T}}$, where $\\bm{U} \\in \\mathbb{R}^{m \\times r}$, $\\bm{G} \\in \\mathbb{R}^{r \\times r}$, and $\\bm{V} \\in \\mathbb{R}^{n \\times r}$ are matrices with i.i.d. $\\mathcal{N}(0,1)$ Gaussian entries. (Note that by $\\bm{X} = \\bm{U} \\bm{G} \\bm{V}^{\\mathsf{T}}$ we are not talking about SVD.)\n",
    "\n",
    "Design and implement a function `LRMC_data_gen` to generate test data. Provide necessary documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using Plots\n",
    "\n",
    "function GaussianGen(m::Int64, n::Int64)\n",
    "    \"\"\"\n",
    "    Generates an mxn array using random entries\n",
    "    sampled from a normal/gaussian distribution.\n",
    "    Each column in the array is also normalised \n",
    "    to be unitary.\n",
    "\n",
    "    :param m: Int64\n",
    "        number of rows for returned matrix\n",
    "    :param n: Int64\n",
    "        number of columns for returned matrix\n",
    "    :return: Array{Float64, m, n}\n",
    "        mxn array with normalised gaussian samples\n",
    "    \"\"\"\n",
    "\n",
    "    A = randn(Float64, m, n)\n",
    "    Norm = zeros(m,n)\n",
    "\n",
    "    for i = 1:n\n",
    "        Norm[:,i] = normalize(A[:,i], 2);\n",
    "    end\n",
    "\n",
    "    return Norm\n",
    "end\n",
    "\n",
    "function Create_linear_operator(Ω)\n",
    "    \"\"\"\n",
    "    Generates a linear operator matrix A using\n",
    "    the masking matrix Omega. A is such that the \n",
    "    following equations are equal.\n",
    "    Y = A * X, Y = Ω .* X where .* is element-wise\n",
    "    multiplication.\n",
    "\n",
    "    :param Ω: Array{Bool, m, n}\n",
    "        Masking matrix Ω \n",
    "    :return: Array{|Ω|, m*n}\n",
    "        mxn array with normalised gaussian samples.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = size(Ω)\n",
    "    idm = Matrix(1I, m*n, m*n)\n",
    "\n",
    "    cardinality = count(i->(i != false), vec(Ω))\n",
    "    A = Array{Int64, 2}(undef, cardinality, m*n)\n",
    "\n",
    "    idx = 1\n",
    "\n",
    "    for i = 1:m*n\n",
    "        if vec(Ω)[i] == true\n",
    "            A[idx, :] = idm[i, :]\n",
    "            idx += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return A\n",
    "end\n",
    "\n",
    "function Observation_samples(X::AbstractArray, m::Int64, n::Int64, samples::Int64)\n",
    "    \"\"\"\n",
    "    Randomly generates a sampled matrix Y using a \n",
    "    matrix X. This is supposed to try and simulate\n",
    "    how in the real world we might be given a matrix\n",
    "    Y with missing entries whose ground truth matrix\n",
    "    is X.\n",
    "\n",
    "    :param X: Array{Any, m, n}\n",
    "        Masking matrix Ω.\n",
    "    :param m: Int64\n",
    "        Number of rows for X matrix.\n",
    "    :param n: Int64\n",
    "        Number of columns for X matrix.\n",
    "    :param samples: Int64\n",
    "        Number of random samples (without replacement)\n",
    "        to retrieve from X to create 'corrupted' matrix Y.\n",
    "        Corresponds to cardinality of Ω i.e. |Ω|. \n",
    "    :return: Array{Any, m, n}, Array{Bool, m, n}\n",
    "        Returns the sampled ('corrupted') matrix Y as well\n",
    "        as the matrix Ω which corresponds to the masking\n",
    "        matrix which was randomly generated to create Y from \n",
    "        X.\n",
    "    \"\"\"\n",
    "\n",
    "    random_indices = sample(randperm(m*n), samples, replace=false)\n",
    "\n",
    "    Ω = zeros(Bool, m, n)\n",
    "    Ω[random_indices] .= true\n",
    "\n",
    "    Y = zeros(m, n)\n",
    "\n",
    "    Y[Ω] = X[Ω]\n",
    "\n",
    "    return Y, Ω\n",
    "end\n",
    "\n",
    "function LRMC_data_gen(m::Int64, n::Int64, r::Int64)\n",
    "    \"\"\"\n",
    "    Creates a matrix with dimensions m, n which will be\n",
    "    of rank r. Original entries will be sampled from\n",
    "    N(0, 1) gaussian and then normalised before being\n",
    "    converted to matrix of rank r.\n",
    "\n",
    "    :param m: Int64\n",
    "        Number of rows for generated matrix.\n",
    "    :param n: Int64\n",
    "        Number of columns for generated matrix.\n",
    "    :param r: Int64\n",
    "        Rank of the generated matrix.\n",
    "    :return: Array{Float64, m, n}\n",
    "        The mxn matrix of rank r generated.\n",
    "    \"\"\"\n",
    "\n",
    "    U = GaussianGen(m, r)\n",
    "    G = GaussianGen(r, r)\n",
    "    V = GaussianGen(n, r)\n",
    "\n",
    "    X = U * G * V'\n",
    "\n",
    "    return X\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Matrix Completion Techniques\n",
    "\n",
    "In the following, the suggested simulation setup is that $m = 32$, $n=48$, $r$ varies in $2:2:8$, and $|\\Omega|/mn$ varies in $\\{1/8,~ 1/6,~ 1/4,~ 1/2\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Alternating Minimization (20%)\n",
    "\n",
    "Design, implement, and run tests for the alternating minimization method for low-rank matrix completion. Use the function name `LRMCRec_AM`. Provide necessary documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For alternating minimization method for low-rank matrix completion, we find the argmin of,\n",
    "$$\n",
    "        \\frac{1}{2}\\underset{P\\in\\mathbb{R}^{m \\times r}, Q\\in\\mathbb{R}^{n \\times r}}{\\operatorname{argmin}} \\| y - \\mathcal{P}_{\\Omega}(PQ^{T})\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "which then can be defined as,\n",
    "\n",
    "$$\n",
    "         \\frac{1}{2}\\underset{P\\in\\mathbb{R}^{m \\times r}, Q\\in\\mathbb{R}^{n \\times r}}{\\operatorname{argmin}} \\| y - \\mathcal{P}_{\\Omega}(\\begin{bmatrix}\n",
    "            p_{1} \\\\\n",
    "            p_{2}  \\\\\n",
    "            p_{3}  \\\\\n",
    "            . \\\\\n",
    "            . \\\\\n",
    "            . \\\\\n",
    "            p_{m} \n",
    "        \\end{bmatrix} [q_{1} q_{2} q_{3} . . . q_{n}])\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "where $ p_{i}$ and $q_{i}$ have a vector length of r.\n",
    "\n",
    "To update P and Q, it is a simple least square problem where\n",
    "$$\n",
    "\\frac{1}{2}\\|y - PQ^T\\|^2_2\n",
    "$$\n",
    "which the least square solution would be,\n",
    "\n",
    "$$\n",
    "Q^T = pinv(P)*y\n",
    "$$\n",
    "\n",
    "then we can update $P$ by transposing the least square problem as shown below,\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\|y-PQ^T\\|^T\n",
    "= \\|y^T - QP^T\\| => \\|y^T - QP^T\\|^2_2\n",
    "$$\n",
    "\n",
    "So for the second least squares problem we solve as such:\n",
    "\n",
    "$$\n",
    "P^T = pinv(Q)*y^T\n",
    "$$\n",
    "\n",
    "Using these least square solution, we can update P and Q to predict X. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using Plots\n",
    "\n",
    "function Least_squares_P(Y, P, Ω)\n",
    "    \"\"\"\n",
    "    Solves the least squares problem using pseudo-inverse\n",
    "    with ||y - P*Q'|| for Q. It does so for each column vector\n",
    "    in Y using the corresp. column vector mask from Ω. Making\n",
    "    sure that the corresp. revealed entries from y is reflected\n",
    "    in the resp. rows & columns in P and Q'.\n",
    "\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix which we are using to estimate P and Q.\n",
    "    :param P: Array{Float64, m, r}\n",
    "        Our current estimate of P for the Y ≈ P * Q'\n",
    "    :param Ω: Array{Float64, m, n}\n",
    "        Masking matrix Ω\n",
    "    :return: Array{Float64, n, r}\n",
    "        Least Squares estimation of Q\n",
    "    \"\"\"\n",
    "\n",
    "    r = size(P, 2)\n",
    "    m = size(Y, 1)\n",
    "    n = size(Y, 2)\n",
    "    \n",
    "    Q = Array{Float64, 2}(undef, n, r)\n",
    "\n",
    "    for i = 1:n\n",
    "        y = Y[:, i]\n",
    "        p = Array{Float64, 2}(undef, m, r)\n",
    "\n",
    "        for j = 1:r\n",
    "            p[:, j] = Ω[:, i] .* P[:, j] \n",
    "        end\n",
    "\n",
    "        Q'[:, i] = pinv(p) * y\n",
    "    end\n",
    "\n",
    "    return Q\n",
    "end\n",
    "\n",
    "function Least_squares_Q(Y, Q, Ω)\n",
    "    \"\"\"\n",
    "    Solves the least squares problem using pseudo-inverse\n",
    "    with ||y' - Q*P'|| for P. It does so for each column vector\n",
    "    in Y using the corresp. column vector mask from Ω. Making\n",
    "    sure that the corresp. revealed entries from y' is reflected\n",
    "    in the resp. rows & columns in Q and P'.\n",
    "\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix which we are using to estimate P and Q.\n",
    "    :param Q: Array{Float64, m, r}\n",
    "        Our current estimate of Q for the Y ≈ P * Q'\n",
    "    :param Ω: Array{Float64, m, n}\n",
    "        Masking matrix Ω\n",
    "    :return: Array{Float64, n, r}\n",
    "        Least Squares estimation of P\n",
    "    \"\"\"\n",
    "\n",
    "    r = size(Q, 2)\n",
    "    m = size(Y, 1)\n",
    "    n = size(Y, 2)\n",
    "\n",
    "    P = Array{Float64, 2}(undef, m, r)\n",
    "\n",
    "    for i = 1:m\n",
    "        y = Y'[:, i]\n",
    "        q = Array{Float64, 2}(undef, n, r)\n",
    "\n",
    "        for j = 1:r\n",
    "            q[:, j] = Ω'[:, i] .* Q[:, j] \n",
    "        end\n",
    "\n",
    "        P'[:, i] = pinv(q) * y\n",
    "    end\n",
    "\n",
    "    return P\n",
    "end\n",
    "\n",
    "function LRMCRec_AM(Y, Ω, r, iters)\n",
    "    \"\"\"\n",
    "    Low-Rank Matrix Completion using Alternating Minimization.\n",
    "\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix with missing values.\n",
    "    :param Ω: Array{Float64, m, n}\n",
    "        Masking matrix Ω.\n",
    "    :param r: Array{Float64, m, r}\n",
    "        The rank of our completed matrix. Used to specify columns of P and Q.\n",
    "    :param iters: Array{Float64, m, r}\n",
    "        Number of iterations to run for.\n",
    "    :return: Array{Float64, m, n}\n",
    "        Our matrix which has had its missing values completed/estimated.\n",
    "    \"\"\"\n",
    "\n",
    "    U, _, _ = svd(Y)\n",
    "\n",
    "    P = U[:, 1:r]\n",
    "    Q = Least_squares_P(Y, P, Ω)\n",
    "\n",
    "    cost_list = []\n",
    "    cost = norm(Y[Ω] - (P*Q')[Ω], 2)^2\n",
    "    append!(cost_list, cost)\n",
    "    \n",
    "    for i = 1:iters-1\n",
    "        Q = Least_squares_P(Y, P, Ω)\n",
    "        P = Least_squares_Q(Y, Q, Ω)\n",
    "\n",
    "        cost = norm(Y[Ω] - (P*Q')[Ω], 2)^2\n",
    "        append!(cost_list, cost)\n",
    "    end\n",
    "\n",
    "    X = P * Q'\n",
    "    return X, cost_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "log = []\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"AM for r=$(rank)\", ylabel=\"cost\", xlabel=\"iteration\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "        iters = 100\n",
    "\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "\n",
    "        X_predict, cost_list = LRMCRec_AM(Y, Ω, rank, iters)\n",
    "        error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "        push!(log, (rank, sample_ratio, error))\n",
    "\n",
    "        println(\"Rank: \", rank, \" samples: \", Ω_sample, \" Error: \", error)\n",
    "        plot!(plotter, cost_list, label=\"|Ω|=$(Ω_sample)\", fmt = :png, dpi=2080)\n",
    "    end\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Iterative Hard Thresholding (IHT) (20%)\n",
    "\n",
    "Design, implement, and run simple tests for the IHT algorithm for low-rank matrix completion. Use the function name `LRMCRec_IHT`. Provide necessary documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IHT algorithm for low-rank matrix recovery is given by\n",
    "$$\n",
    "X^{l+1} = H_{rank-r}(X^{l}+τA^{*}(y-A(X^{l})))\n",
    "$$\n",
    "\n",
    "where $H_{rank-r} $ is the best rank r approximation given by $\\sum_{i=1}^{r} \\sigma_{i}u_{i}v_{i}^{T}$ \n",
    "\n",
    "The linear map $A$ is the projection operator to a vector of revealed entries only. \n",
    "\n",
    "$A(X)$ can be represented in code as $X[Ω]$ within Julia and $y$ can be represented in code as $Y[Ω]$.\n",
    "\n",
    "$A^{*}$ is the adjoint operator of the linear map $A$ where it maps the projected vector back to a $(m,n)$ matrix. The iterations can hence be represented in code as such : \n",
    "\n",
    "            m,n = size(Y)\n",
    "            temp = zeros(m,n)\n",
    "            temp[Ω] = Y[Ω] - X[Ω]\n",
    "            X = rank_r_approx(X + τ*(temp), r)\n",
    "where the variable 'temp' is equivalent to $A^{*}(y-A(X^{l}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function r_rank_approx(M::AbstractArray, r::Int)\n",
    "    \"\"\"\n",
    "    Hard-Thresholding using SVD and the r largest singular values.\n",
    "    \n",
    "    :param M: Array{m, n}\n",
    "        Input matrix of size mxn to perform hard-thresholding on.\n",
    "    :param r: Int64\n",
    "        Rank of the matrix we are performing matrix completion on.\n",
    "    :return: Array{Float64, m, r}\n",
    "        Matrix which has had hard-thresholding performed on its singular values.\n",
    "    \"\"\"\n",
    "\n",
    "    U, Σ, V = svd(M)\n",
    "    N = length(Σ)\n",
    "\n",
    "    for i = r+1:N\n",
    "        Σ[i] = 0\n",
    "    end\n",
    "\n",
    "    return U * Diagonal(Σ) * V'\n",
    "end\n",
    "\n",
    "function LRMCRec_IHT(Y, Ω, r::Int, τ::Float64, iters::Int)\n",
    "    \"\"\"\n",
    "    Iterative Hard Thresholding for low-rank-approximation completion problem.\n",
    "    \n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix with missing values.\n",
    "    :param Ω: Array{Bool, m, n}\n",
    "        Masking matrix Ω, corresponds to revealed entries in Y.\n",
    "    :param r: Int64\n",
    "        Rank of the generated matrix.\n",
    "    :param τ: Float64\n",
    "        Step size.\n",
    "    :param iters: Int\n",
    "        Number of iterations to run for.\n",
    "    :return: Array{Float64, m, n}\n",
    "        Our matrix which has had its missing values completed/estimated.\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = size(Y)\n",
    "    X = copy(Y)\n",
    "    \n",
    "    temp = zeros(m,n)   \n",
    "    cost_list = []\n",
    "    cost = norm(Y[Ω] - X[Ω], 2)^2\n",
    "    append!(cost_list, cost)\n",
    "\n",
    "    for i = 1:iters\n",
    "        # Iteration algorithm to reduce objective error \n",
    "        temp[Ω] = Y[Ω] - X[Ω]\n",
    "        X = r_rank_approx(X + τ*(temp), r)\n",
    "        \n",
    "        #Calculate current cost\n",
    "        cost = norm(Y[Ω] - X[Ω], 2)^2\n",
    "        append!(cost_list, cost)\n",
    "\n",
    "    end \n",
    "    return X, cost_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "log = []\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"IHT for r=$(rank)\", ylabel=\"final error\", xlabel=\"τ\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "        info_τ = []\n",
    "        info_error = []\n",
    "\n",
    "        for τ = 0.01:0.1:1.0\n",
    "            iters = 100\n",
    "            X_predict, _ = LRMCRec_IHT(Y, Ω, rank, τ, iters)\n",
    "            error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "            push!(info_τ, τ)\n",
    "            push!(info_error, error)\n",
    "\n",
    "            push!(log, (rank, sample_ratio, τ, error))\n",
    "        end\n",
    "\n",
    "        plot!(plotter, info_τ, info_error, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=520)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finding the best low rank approximation of $X$, $\\hat{X}$ with rank($\\hat{X}$) = rank($X$) gives the best approximation of $X$.\n",
    "\n",
    "In general, the two following points hold true.\n",
    "\n",
    "\n",
    "\n",
    "* For a matrix with fixed-size, a larger number of revealed entries results in a lower final error between $\\hat{X}$ and $X$\n",
    "* For a matrix with fixed-size, a lower rank of $X$ will result in a lower final error between $\\hat{X}$ and $X$\n",
    "* For a matrix with a higher rank, we require a sufficiently larger amount of revealed entries such that the final error can be significantly reduced.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"IHT for r=$(rank)\", ylabel=\"cost\", xlabel=\"iterations\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "\n",
    "        τ = 1.0\n",
    "        iters = 100\n",
    "        X_predict, cost_list = LRMCRec_IHT(Y, Ω, rank, τ, iters)\n",
    "        error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "        plot!(plotter, cost_list, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=520)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function converges to a sufficiently small value in all cases regardless of the rank of $X$ and the number of revealed entries |Ω|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Iterative Shrinkage-Thresholding Algorithm (ISTA) (25%)\n",
    "\n",
    "Design, implement, and run simple tests for ISTA (to solve the Lasso formulation) for low-rank matrix completion. Use the function name `LRMCRec_ISTA`. Provide necessary documentation. Use simulations to discuss the choice of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Iterative Shrinkage-Thresholding Algorithm for low rank matrix completion, we start with the lasso formulation,\n",
    "$$\n",
    "        \\frac{1}{2}\\|y - \\mathbb{A}(X)\\|^2_2 + \\lambda\\|X\\|_{tr}\n",
    "$$\n",
    "Where the $\\|X\\|_{tr}$ is the nuclear norm of X which is,\n",
    "\n",
    "$$\n",
    "        \\|X\\|_{tr} = \\sum_{(i,j) \\in \\Omega}^r\\sigma_i(B)\n",
    "$$\n",
    "\n",
    "$$\n",
    "            \\partial \\|X\\|_{tr} = \\sum_{i = 1}^{min(m,n)} sign(\\sigma_i)u_iv_i^T    \n",
    "$$\n",
    "\n",
    "The proximal operator of the nuclear norm is,\n",
    "$$\n",
    "\\underset{X}{\\operatorname{argmin}}\\|X\\|_{tr} + \\frac{1}{2\\gamma}\\|X-Z\\|^2_F\n",
    "$$\n",
    "$$\n",
    "=\\sum_{i = 1}^{min(m,n)}\\eta(\\sigma_i;\\gamma)u_iv_i^T\n",
    "$$\n",
    "\n",
    "Let $f(X) = \\frac{1}{2}\\|y - \\mathbb{A}(X)\\|^2_2 $.\n",
    "This can be used if we approximate the objective function as,\n",
    "$$\n",
    "f(X) + \\lambda\\|X\\|_{tr}\n",
    "\\approx f(X^l) + <X-X^l,\\nabla f(X^l)> + \\frac{1}{2\\tau^l}\\|X-X^l\\|^2_2 + \\lambda\\|X\\|_{tr}\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2\\tau^l}\\|X-(X^l - \\tau^l\\nabla f(X^l))\\|^2_2 + \\lambda\\|X\\|_{tr} + c\n",
    "$$\n",
    "This is very similar to the soft thresholding function given above for the proximal operator of the nuclear norm. Therefore the iteration is given by,\n",
    "$$\n",
    "X^{l+1} = \\eta_{\\sigma}(X^l + \\tau^l\\mathbb{A}^*(y-\\mathbb{A}(X^l));\\lambda\\tau^l)\n",
    "$$\n",
    "\n",
    "Since the linear operator for our case is $\\mathcal{P}_{\\Omega}(\\bm{X})$, the iteration will be as shown,\n",
    "\n",
    "$$\n",
    "X^{l+1} = S_{\\lambda\\tau}(X+\\tau(\\mathcal{P}_{\\Omega}(Y)-\\mathcal{P}_{\\Omega}(X)))\n",
    "$$\n",
    "\n",
    "Therefore, we can use this soft thresholding function to update our X for each iteration in the code.\n",
    "\n",
    "Since $\\nabla g(B) = \\mathcal{P}_{\\Omega}(Y)-\\mathcal{P}_{\\Omega}(X)$, is Lipschitz continuous, that is the function is uniform continous, we set the step size $\\tau$ equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using Plots\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "function nuclear_norm(X)\n",
    "    \"\"\"\n",
    "    Computes the nuclear norm of X using the singular values.\n",
    "    \n",
    "    :param X: Array{Float64, m, n}\n",
    "        Input matrix to compute nuclear norm on.\n",
    "    :return: Float64\n",
    "        Nuclear norm of X\n",
    "    \"\"\"\n",
    "\n",
    "    _, S, _ = svd(X)\n",
    "    return sum(S)\n",
    "end\n",
    "\n",
    "function current_cost(X, Y, Ω, λ)\n",
    "    \"\"\"\n",
    "    Computes the Cost function (Lasso) for X and Y.\n",
    "    \n",
    "    :param X: Array{Float64, m, n}\n",
    "        Input matrix meant to symbolise the matrix we've predicted.\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Input matrix meant to symbolise the matrix we're trying to recreate.\n",
    "    :param Ω: Array{Bool, m, n}\n",
    "        Masking matrix Ω, corresponds to revealed entries in Y.\n",
    "    :param λ: Float64\n",
    "        Integer value for the regularization parameter in Lasso.\n",
    "    :return: Float64\n",
    "        Cost i.e. (1/2) * ||Pr(Y) - Pr(X)||_2 + λ||Pr(X)||_*\n",
    "    \"\"\"\n",
    "    \n",
    "    return 0.5 * norm(X[Ω] - Y[Ω])^2 + λ * nuclear_norm(X)\n",
    "end\n",
    "\n",
    "function Singular_value_soft_threshold(X, λ)\n",
    "    \"\"\"\n",
    "    Computes the Soft-Threshold on the singular values of X using \n",
    "    regulariation parameter λ.\n",
    "    \n",
    "    :param X: Array{Float64, m, n}\n",
    "        Input matrix to compute on.\n",
    "    :param λ: Float64\n",
    "        Integer value for the regularization parameter in Lasso.\n",
    "    :return: Array{Float64, m, n}\n",
    "        Matrix which has had Soft-Thresholding \n",
    "        applied on its singular values.\n",
    "    \"\"\"\n",
    "\n",
    "    U, S, V = svd(X)\n",
    "    threshold = max.(S .- λ, 0.0)\n",
    "    return U * Diagonal(threshold) * V'\n",
    "end\n",
    "\n",
    "# Low Rank Matrix Completion Iterative Shrinkage Thresholding Algorithm.\n",
    "\n",
    "function LRMCRec_ISTA(Y, Ω, λ, iterations)\n",
    "    \"\"\"\n",
    "    Computes the Iterative-Soft-Thresholding-Algorithm for the purpose\n",
    "    of Low Rank Matrix Completion for a observation matrix Y. Uses Lasso \n",
    "    in order to create sparse solutions.\n",
    "    \n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix with missing values.\n",
    "    :param Ω: Array{Bool, m, n}\n",
    "        Masking matrix Ω, corresponds to revealed entries in Y from\n",
    "        some ground-truth matrix we don't know.\n",
    "    :param λ: Float64\n",
    "        Regularization parameter\n",
    "    :param iters: Int\n",
    "        Number of iterations to run for.\n",
    "    :return: Array{Float64, m, n}\n",
    "        Our matrix which has had its missing values completed/estimated.\n",
    "    \"\"\"\n",
    "\n",
    "    X = copy(Y)\n",
    "\n",
    "    cost_list = zeros(iterations + 1)\n",
    "    cost_list[1] = current_cost(X, Y, Ω, λ)\n",
    "\n",
    "    for i=1:iterations\n",
    "        X[Ω] = Y[Ω]\n",
    "        X = Singular_value_soft_threshold(X, λ)\n",
    "\n",
    "        cost_list[i+1] = current_cost(X, Y, Ω, λ)\n",
    "    end\n",
    "\n",
    "    return X, cost_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "log = []\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"ISTA for r=$(rank)\", ylabel=\"final error\", xlabel=\"λ\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "        info_λ = []\n",
    "        info_error = []\n",
    "\n",
    "        for λ = 0.01:0.1:1.0\n",
    "            iters = 100\n",
    "            X_predict, cost_list = LRMCRec_ISTA(Y, Ω, λ, iters)\n",
    "            error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "            push!(info_λ, λ)\n",
    "            push!(info_error, error)\n",
    "\n",
    "            push!(log, (rank, sample_ratio, λ, error))\n",
    "        end\n",
    "\n",
    "        plot!(plotter, info_λ, info_error, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=2080)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"ISTA for r=$(rank)\", ylabel=\"cost\", xlabel=\"iterations\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "\n",
    "        λ = 0.01\n",
    "        iters = 100\n",
    "        X_predict, cost_list = LRMCRec_ISTA(Y, Ω, λ, iters)\n",
    "        error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "        plot!(plotter, cost_list, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=2080)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Lasso-ADMM (25%)\n",
    "\n",
    "Design, implement, and run simple tests for an ADMM algorithm (to solve the Lasso formulation) for low-rank matrix completion. Use the function name `LRMCRec_ADMM`. Provide necessary documentation. Compare ADMM and ISTA in terms of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso formulation is written as:\n",
    "$$\n",
    "argmin_{x} \\frac{1}{2}||y - Ax||_{2}^{2} + \\lambda||x||_{1}\n",
    "$$\n",
    "Since ADMM algorithm will be used for low rank matrix completion, we can interpret the $Ax$ of the lasso problem as the linear mapping of the revealed entries of x (our estimation matrix) indexed by $\\Omega$.\n",
    "\n",
    "Given that the corresponding ADMM iterations are:\n",
    "$$\n",
    "x^{k+1} = arg min_x \\frac{1}{2}||y - Ax||_{2}^{2} + \\frac{\\rho}{2}||x - z_{k} + \\frac{v_{k}}{\\rho}||_{2}^{2}\n",
    "$$\n",
    "$$\n",
    "z^{k+1} = arg min_z \\lambda ||z||_{1} + \\frac{\\rho}{2}||x_{k+1} - z + \\frac{v_{k}}{\\rho}||_{2}^{2}\n",
    "$$\n",
    "$$\n",
    "v^{k+1} = v_{k} + \\rho (x^{k+1} - z^{k+1})\n",
    "$$\n",
    "\n",
    "For $x^{k+1}$, we can calculate the closed form solutions by setting $\\nabla f(x)=0$, resulting in:\n",
    "$$\n",
    "x = (A^{T}A + \\rho)^{-1} (A^{T}y + \\rho z^{k} - v^{k})\n",
    "$$\n",
    "\n",
    "Here $(A^{T}A + \\rho)^{-1}$ is equal to the reciprocal of $A^{T}A + \\rho$, i.e $\\frac{1}{element}$, as it is a diagonal matrix. $A^{T} y = Y$ so it results in element wise division by each corresponding element in $\\Omega + \\rho$ where $\\Omega$ is our matrix of revealed entries.\n",
    "Thus, we have:\n",
    "$$\n",
    "x^{k+1} = (Y + \\rho*(Z-V))⊙(\\Omega + \\rho)\n",
    "$$\n",
    "where ⊙ is element-wise division\n",
    "\n",
    "To find the closed form solution of $z^{k+1}$, we first consider the solution for the ISTA algorithm. \n",
    "\n",
    "$$\n",
    "X^{*} = arg min_{x}||X|| + \\frac{1}{2\\gamma}||X-Z||^{2}_{F}\n",
    "$$\n",
    "$$\n",
    "= \\Sigma_{i=1}\\eta(\\sigma_{i};\\gamma)u_{i}v_{i}^{T} \n",
    "$$\n",
    "where $\\eta$ is the soft thresholding function.\n",
    "Comparing this to the iterations of $z^{k+1}$, we see that it is of similar form and thus can compose:\n",
    "$$\n",
    "z^{k+1} = \\eta(X+V; \\frac{\\lambda}{\\rho})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "function Singular_value_soft_threshold(X, λ)\n",
    "    \"\"\"\n",
    "    Generates a matrix which has had Soft-Thresholding calculated\n",
    "    on its singular values using SVD.\n",
    "\n",
    "    :param X: Array{Float64, m, n}\n",
    "        Matrix to calculate singular value soft thresholding on.\n",
    "    :param λ: Float64\n",
    "        Regularization parameter.\n",
    "    :return: Array{Float64, m, n}\n",
    "        Matrix which has had soft-threshold applied on its singular\n",
    "        value.\n",
    "    \"\"\"\n",
    "\n",
    "    U, S, V = svd(X)\n",
    "    threshold = max.(S .- λ, 0.0)\n",
    "    return U * Diagonal(threshold) * V'\n",
    "end\n",
    "\n",
    "\n",
    "function nuclear_norm(X)\n",
    "    \"\"\"\n",
    "    Calculates the L1/Nuclear norm of a matrix\n",
    "\n",
    "    :param X: Array{Float64, m, n}\n",
    "        Input Matrix to compute nuclear norm of.\n",
    "    :return: Float64\n",
    "        Nuclear norm of Matrix X\n",
    "    \"\"\"\n",
    "\n",
    "    _, S, _ = svd(X)\n",
    "    return sum(S)\n",
    "end\n",
    "\n",
    "\n",
    "function current_cost(X, Y, Ω, λ)\n",
    "    \"\"\"\n",
    "    Calculates the lasso formulation\n",
    "\n",
    "    :param X: Array{Float64, m, n}\n",
    "        our estimated matrix that we try to complete.\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        the groundtruth masked matrix.\n",
    "    :param Ω: Array{Float64, m, n}\n",
    "        Matrix of revealed indices in Y.\n",
    "    :param λ: Float64\n",
    "        Regularization parameter for the lasso problem.\n",
    "    :return: Float64\n",
    "        Cost i.e. (1/2) * ||Pr(Y) - Pr(X)||_2 + λ||Pr(X)||_*\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    return 0.5 * norm(X[Ω] - Y[Ω])^2 + λ * nuclear_norm(X)\n",
    "end\n",
    "\n",
    "function LRMCRec_ADMM(Y, Ω, λ, ρ, iters, error_diff_tresh)\n",
    "    \"\"\"\n",
    "    Solve the lasso problem for low rank matrix completion with the ADMM algorithm.\n",
    "\n",
    "    :param Y: Array{Float64, m, n}\n",
    "        Our observation matrix with missing entries\n",
    "    :param Ω: Array{Bool, m, n}\n",
    "        Matrix of revealed entries from Y.\n",
    "    :param λ: Float64\n",
    "        Regularization parameter for the lasso problem.\n",
    "    :param ρ: Float64\n",
    "        Penalty parameter for the ADMM algorithm.\n",
    "    :param iters: Int64\n",
    "        Maximum number of iterations for the algorithm before breaking\n",
    "    :param error_diff_tresh: Float64\n",
    "        Threshold comparing the current loss and previous loss to determine whether a significant update has been made.\n",
    "    :return: Array{Float64, m, n}\n",
    "        The matrix which we got using low rank matrix completion with the ADMM algorithm for Lasso problems.\n",
    "    \"\"\"\n",
    "\n",
    "    X = copy(Y)\n",
    "    m, n = size(X)\n",
    "    \n",
    "    Z = zeros(Float64, m, n)\n",
    "    V = zeros(Float64, m, n)\n",
    "\n",
    "    cost_past = Inf\n",
    "    cost = current_cost(X, Y, Ω, λ)\n",
    "    counter = 0\n",
    "    cost_list = []\n",
    "    push!(cost_list, cost)\n",
    "    \n",
    "    while counter <= iters\n",
    "        X = (Y + ρ*(Z - V)) ./ (Ω .+ ρ)\n",
    "        Z = Singular_value_soft_threshold(X+V, λ/ρ)\n",
    "        V += X\n",
    "        V -= Z\n",
    "\n",
    "        cost = current_cost(X, Y, Ω, λ)\n",
    "        push!(cost_list, cost)\n",
    "        counter += 1\n",
    "\n",
    "        if (cost_past - cost <= error_diff_tresh)\n",
    "            break\n",
    "        else\n",
    "            cost_past = cost\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return X, counter, cost_list\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing lambda vs Final Error\n",
    "\n",
    "using Plots\n",
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "log = []\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"ADMM for r=$(rank)\", ylabel=\"final error\", xlabel=\"λ\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "        info_λ = []\n",
    "        info_error = []\n",
    "\n",
    "        for λ = 0.01:0.01:0.25\n",
    "            iters = 100\n",
    "            ρ = 0.5\n",
    "            error_diff_thresh = 0.00001\n",
    "            X_predict, counter, cost_list = LRMCRec_ADMM(Y, Ω, λ, ρ, iters, error_diff_thresh)\n",
    "            error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "            push!(info_λ, λ)\n",
    "            push!(info_error, error)\n",
    "\n",
    "            push!(log, (rank, sample_ratio, λ, error))\n",
    "        end\n",
    "\n",
    "        plot!(plotter, info_λ, info_error, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=2080)\n",
    "        print(info_error[end])\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the error versus $\\lambda$, the plots reveal the minimum error occurs approximately around and before 0.05 across the various ranks. We use this as a guideline to determine our $\\lambda$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing error vs rho\n",
    "\n",
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "log = []\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"ADMM for r=$(rank)\", ylabel=\"final error\", xlabel=\"ρ\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "        info_ρ = []\n",
    "        info_error = []\n",
    "\n",
    "        for ρ = 0.01:0.01:1\n",
    "            iters = 100\n",
    "            λ = 0.025\n",
    "            error_diff_thresh = 0.00001\n",
    "            X_predict, counter, cost_list = LRMCRec_ADMM(Y, Ω, λ, ρ, iters, error_diff_thresh)\n",
    "            error = norm(X_predict - X) / norm(X)\n",
    "\n",
    "            push!(info_ρ, ρ)\n",
    "            push!(info_error, error)\n",
    "\n",
    "            push!(log, (rank, sample_ratio, ρ, error))\n",
    "        end\n",
    "\n",
    "        plot!(plotter, info_ρ, info_error, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=2080)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the final error to $\\rho$, the plots reveal that there is no definite relationship between the two variables as the final error is approximately uniform, independent of $\\rho$. $\\rho$ is an arbitrary penalty constant, which is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing performance of the ADMM algorithm\n",
    "m, n = 32, 48\n",
    "\n",
    "samples = [1/8, 1/6, 1/4, 1/2]\n",
    "\n",
    "plot_arr = Plots.Plot{Plots.GRBackend}[]\n",
    "\n",
    "for rank = 2:2:8\n",
    "    X  = LRMC_data_gen(m, n, rank)\n",
    "    plotter = plot(title=\"ADMM for r=$(rank)\", ylabel=\"cost\", xlabel=\"iterations\")\n",
    "\n",
    "    for sample_idx = 1:length(samples)\n",
    "        sample_ratio = samples[sample_idx]\n",
    "        Ω_sample = Int(m*n * sample_ratio)\n",
    "\n",
    "        Y, Ω = Observation_samples(X, m, n, Ω_sample)\n",
    "\n",
    "        λ = 0.025\n",
    "        ρ = 1\n",
    "        error_diff_thresh = 0.00001\n",
    "        iters = 100\n",
    "        X_predict, counter, cost_list = LRMCRec_ADMM(Y, Ω, λ, ρ, iters, error_diff_thresh)\n",
    "\n",
    "        plot!(plotter, cost_list, label=\"|Ω|=$(Ω_sample)\",fmt = :png, dpi=2080)\n",
    "        ln = length(cost_list)\n",
    "    end\n",
    "\n",
    "    push!(plot_arr, plotter)\n",
    "end\n",
    "\n",
    "plot(plot_arr[1],plot_arr[2],plot_arr[3],plot_arr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADMM algorithm performs a sharp convergence for all of the various ranks and omega values. For each rank and omega, the cost approximately converges in less than 25 iterations. \n",
    "\n",
    "Comparedd to the performance to the ISTA algorithm, the ADMM demonstrates superior performance with a quicker convergence across all ranks and cardinalities. For example, at rank = 8 and cardinality = 768, the ADMM algorithm converges at approximately 50 iterations. The ISTA algorithm converges at approximately 100 iterations.\n",
    "\n",
    "Compared to the ISTA algorithm, the ADMM demonstrates a sharper, or quicker, convergence, most likely due to the dual ascent approach stemming from the Lagrangian multipliers, not found in the ISTA algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight\n",
    "\n",
    "Please list a couple of highlights of your coursework that may impress your markers.\n",
    "\n",
    "1. Our functions are all documented like you would find in the standard libary of Python and many other popular Python projects you might find on GitHub.\n",
    "2. By using a masking matrix Ω instead of a Linear Operator 'A', we speed up our calculations quite a bit. It also enables us to write very simple code calculations in comparison to if we had used 'A'. It took considerable work to translate the algorithms to a code friendly format.\n",
    "3. We also give short mathematical explanations to have we arrived at the code written from the definitions from the lecture notes.\n",
    "4. We learned where we could remove extraneous constants, such as τ from ISTA (the step-size) doesn't matter too much since it is essentially fixed. Instead it can be 'clumped' together with λ for ISTA.\n",
    "5. Our plots are concise, labeled with Legend and Axes, as well as show good detailed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
